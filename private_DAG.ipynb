{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO/WWIVS/DfqbOdxkH5TGbR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# README: "],"metadata":{"id":"1xSd1iYWRBYw"}},{"cell_type":"markdown","source":["This notebook contains the private version of DAG-WGAN. Anyone can train the model using the GPUs from Google Colab VMs. \n","\n","It can be easily run by toggling args.differentialPrivacy to True or False in Private_Main.py. Feel free to play around with the hyper-parameters as there is plenty of tweeking still to be done.\n","\n","edits: @calmac\n","\n","last: 13/10/22"],"metadata":{"id":"4bjIoWHGRx_H"}},{"cell_type":"markdown","source":["#========================================="],"metadata":{"id":"8ut9_QesRFYx"}},{"cell_type":"markdown","source":["# Differentially private DAG training"],"metadata":{"id":"V3MGb3byKDRv"}},{"cell_type":"markdown","source":["## Utils.py"],"metadata":{"id":"IyjNXMiAnvPZ"}},{"cell_type":"code","source":["\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Thu Nov 12 15:13:21 2020\n","@author: Hristo Petkov\n","\"\"\"\n","\n","\"\"\"\n","@inproceedings{yu2019dag,\n","  title={DAG-GNN: DAG Structure Learning with Graph Neural Networks},\n","  author={Yue Yu, Jie Chen, Tian Gao, and Mo Yu},\n","  booktitle={Proceedings of the 36th International Conference on Machine Learning},\n","  year={2019}\n","}\n","@inproceedings{xu2019modeling,\n","  title={Modeling Tabular data using Conditional GAN},\n","  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},\n","  booktitle={Advances in Neural Information Processing Systems},\n","  year={2019}\n","}\n","\"\"\"\n","import torch\n","import os\n","import math\n","import numpy as np\n","import torch.nn as nn\n","import networkx as nx\n","import scipy.linalg as slin\n","import torch.nn.functional as F\n","from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.feature_selection import SelectFromModel\n","from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n","from networkx.convert_matrix import from_numpy_matrix\n","from matplotlib import pyplot as plt\n","from torch.utils.data.dataset import TensorDataset\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","# from FullDataPreProcessor import FullDataPreProcessor\n","\n","# AAE utility functions\n","\n","def num_categories(labels):\n","    return len(set(labels))\n","\n","def my_softmax(input, axis=1):\n","    trans_input = input.transpose(axis, 0).contiguous()\n","    soft_max_1d = F.softmax(trans_input, dim=-1)\n","    return soft_max_1d.transpose(axis, 0)\n","\n","def relu(x, derivative=False, alpha=0.1):\n","    rel = x * (x > 0)\n","    if derivative:\n","        return (x > 0)*1\n","    return rel\n","\n","def preprocess_adj_new(adj, device):\n","    adj_normalized = (torch.eye(adj.shape[0]).double().to(device) - (adj.transpose(0,1)).to(device))\n","    return adj_normalized\n","\n","def preprocess_adj_new1(adj, device):\n","    adj_normalized = torch.inverse(torch.eye(adj.shape[0]).double().to(device) - adj.transpose(0,1).to(device))\n","    return adj_normalized\n","\n","def matrix_poly(matrix, d, device):\n","    x = torch.eye(d).double().to(device) + torch.div(matrix, d).to(device)\n","    return torch.matrix_power(x, d)\n","\n","# compute constraint h(A) value\n","def _h_A(A, m, device):\n","    expm_A = matrix_poly(A*A, m, device)\n","    h_A = torch.trace(expm_A) - m\n","    return h_A\n","\n","def build_phi(w, totalcount):\n","    phi = w[totalcount:].reshape(-1,1)\n","    return phi\n","\n","def build_W(w, d,totalcount):\n","    # build w\n","    w1 = np.zeros([d, d])\n","    lower_index = np.tril_indices(d, -1)\n","    w1[lower_index] = w[:totalcount]\n","\n","    return w1 + w1.T - np.diag(w1.diagonal())\n","\n","def build_w_inv(A, phi, d,totalcount):\n","    # build w\n","    w1 = np.zeros([d, d])\n","    for i in range(d-1):\n","        for j in range(d-1):\n","            if ((relu(phi[j]-phi[i])>1e-8)):\n","                w1[i,j] = A[i,j]/relu(phi[j]-phi[i])\n","            else:\n","                w1[i,j] = 0\n","    w = (w1 + w1.T)/2.\n","    wnew = np.zeros(totalcount)\n","    lower_index = np.tril_indices(d, -1)\n","    wnew[:totalcount] = w[lower_index]\n","\n","    return wnew\n","\n","def to_categorical(y, num_columns):\n","    \"\"\"Returns one-hot encoded Variable\"\"\"\n","    y_cat = np.zeros((y.shape[0], num_columns))\n","    y_cat[range(y.shape[0]), y] = 1.0\n","\n","    return Variable(torch.FloatTensor(y_cat))\n","\n","def pns_(model_adj, dataloader, num_neighbors, thresh):\n","    \"\"\"Preliminary neighborhood selection\"\"\"\n","    #x_train, _ = train_data.sample(train_data.num_samples)\n","    #x_test, _ = test_data.sample(test_data.num_samples)\n","    #x = np.concatenate([x_train.detach().cpu().numpy(), x_test.detach().cpu().numpy()], 0)\n","    x = dataloader.dataset.tensors[0].squeeze()\n","    #print(x.shape)\n","\n","    num_samples = x.shape[0]\n","    num_nodes = x.shape[1]\n","    print(\"PNS: num samples = {}, num nodes = {}\".format(num_samples, num_nodes))\n","    for node in range(num_nodes):\n","        print(\"PNS: node \" + str(node))\n","        x_other = np.copy(x)\n","        x_other[:, node] = 0\n","        reg = ExtraTreesRegressor(n_estimators=500)\n","        reg = reg.fit(x_other, x[:, node])\n","        selected_reg = SelectFromModel(reg, threshold=\"{}*mean\".format(thresh), prefit=True,\n","                                       max_features=num_neighbors)\n","        mask_selected = selected_reg.get_support(indices=False).astype(np.float)\n","\n","        model_adj[:, node] *= mask_selected\n","\n","    return model_adj\n","\n","def nll_catogrical(preds, target, add_const = False, eps=1e-16):\n","    '''compute the loglikelihood of discrete variables\n","    '''\n","    loss = nn.CrossEntropyLoss(reduction='sum')\n","    output = loss(preds, torch.argmax(target,1))\n","    return output   \n","\n","def nll_gaussian(preds, target, variance, add_const=False):\n","    \n","    mean1 = preds\n","    mean2 = target\n","    \n","    neg_log_p = variance + torch.div(torch.pow(mean1 - mean2, 2), 2.*np.exp(2. * variance))\n","    \n","    if add_const:\n","        const = 0.5 * torch.log(2 * torch.from_numpy(np.pi) * variance)\n","        neg_log_p += const\n","            \n","    return neg_log_p.sum() / (target.size(0))\n","    \n","def kl_gaussian_sem(logits):\n","    mu = logits\n","    kl_div = mu * mu\n","    kl_sum = kl_div.sum()\n","    return (kl_sum / (logits.size(0)))*0.5\n","\n","# def nll_catogrical(preds, target, add_const = False,  eps = 1e-20):\n","#     '''compute the loglikelihood of discrete variables\n","#     '''\n","#     #BCE = F.binary_cross_entropy(preds, target, size_average=False) / target.shape[0]\n","#     BCE = torch.sum(target * torch.log(preds + eps), dim=1).mean()\n","#     return BCE\n","\n","def kl_categorical(preds, num_cats, eps=1e-16):\n","    # KL Divergence = entropy (logits) - cross_entropy(logits, uniform log-odds)\n","    q_y = F.softmax(preds, dim=-1) # convert logits values to probabilities\n","    kl1 = q_y * torch.log(q_y + eps) # entropy (self.latent)\n","    kl2 = q_y * np.log((1.0/num_cats) + eps) # cross_entropy(logits, uniform log-odds)\n","    KL_divergence = torch.sum(torch.sum(kl1 - kl2, 2),1).mean()\n","    return KL_divergence\n","     \n","\n","def sample_gumbel(shape, eps=1e-10):\n","    \"\"\"\n","    NOTE: Stolen from https://github.com/pytorch/pytorch/pull/3341/commits/327fcfed4c44c62b208f750058d14d4dc1b9a9d3\n","    Sample from Gumbel(0, 1)\n","    based on\n","    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n","    (MIT license)\n","    \"\"\"\n","    U = torch.rand(shape).float()\n","    return - torch.log(eps - torch.log(U + eps))\n","\n","\n","def gumbel_softmax_sample(logits, tau=1, eps=1e-10):\n","    \"\"\"\n","    NOTE: Stolen from https://github.com/pytorch/pytorch/pull/3341/commits/327fcfed4c44c62b208f750058d14d4dc1b9a9d3\n","    Draw a sample from the Gumbel-Softmax distribution\n","    based on\n","    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb\n","    (MIT license)\n","    \"\"\"\n","    gumbel_noise = sample_gumbel(logits.size(), eps=eps)\n","    if logits.is_cuda:\n","        gumbel_noise = gumbel_noise.cuda()\n","    y = logits + Variable(gumbel_noise).double()\n","    return my_softmax(y / tau, axis=-1)\n","\n","def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10):\n","    \"\"\"\n","    NOTE: Stolen from https://github.com/pytorch/pytorch/pull/3341/commits/327fcfed4c44c62b208f750058d14d4dc1b9a9d3\n","    Sample from the Gumbel-Softmax distribution and optionally discretize.\n","    Args:\n","      logits: [batch_size, n_class] unnormalized log-probs\n","      tau: non-negative scalar temperature\n","      hard: if True, take argmax, but differentiate w.r.t. soft sample y\n","    Returns:\n","      [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n","      If hard=True, then the returned sample will be one-hot, otherwise it will\n","      be a probability distribution that sums to 1 across classes\n","    Constraints:\n","    - this implementation only works on batch_size x num_features tensor for now\n","    based on\n","    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n","    (MIT license)\n","    \"\"\"\n","    y_soft = gumbel_softmax_sample(logits, tau=tau, eps=eps)\n","    if hard:\n","        shape = logits.size()\n","        _, k = y_soft.data.max(-1)\n","        # this bit is based on\n","        # https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5\n","        y_hard = torch.zeros(*shape)\n","        if y_soft.is_cuda:\n","            y_hard = y_hard.cuda()\n","        y_hard = y_hard.zero_().scatter_(-1, k.view(shape[:-1] + (1,)), 1.0)\n","        # this cool bit of code achieves two things:\n","        # - makes the output value exactly one-hot (since we add then\n","        #   subtract y_soft value)\n","        # - makes the gradient equal to y_soft gradient (since we strip\n","        #   all other gradients)\n","        y = Variable(y_hard - y_soft.data) + y_soft\n","    else:\n","        y = y_soft\n","    return y\n","\n","#Plotting the DAG\n","#Borrowed from Causalnex Documentation\n","#https://causalnex.readthedocs.io/en/latest/03_tutorial/plotting_tutorial.html\n","def draw_dag(graph, data_type, columns = None):\n","    \n","    final_DAG = from_numpy_matrix(graph, create_using=nx.DiGraph)\n","    \n","    if data_type == 'real':\n","        final_DAG = nx.relabel_nodes(\n","            final_DAG, dict(zip(list(range(graph.shape[0])), columns)))\n","    final_DAG.remove_nodes_from(list(nx.isolates(final_DAG)))\n","    \n","    print('FINAL DAG')\n","    print(final_DAG.adj)\n","    \n","    write_dot(final_DAG,'test.dot')\n","    \n","    fig = plt.figure(figsize=(15, 8))  # set figsize\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.set_facecolor(\"#001521\")  # set backgrount\n","\n","    pos = graphviz_layout(final_DAG, prog=\"dot\")\n","\n","    # add nodes to figure\n","    nx.draw_networkx_nodes(\n","        final_DAG,\n","        pos,\n","        node_shape=\"H\",\n","        node_size=1000,\n","        linewidths=3,\n","        edgecolors=\"#4a90e2d9\",\n","    )\n","    \n","    # add labels\n","    nx.draw_networkx_labels(\n","        final_DAG,\n","        pos,\n","        font_color=\"#FFFFFFD9\",\n","        font_weight=\"bold\",\n","        font_family=\"Helvetica\",\n","        font_size=10,\n","    )\n","    \n","    # add edges\n","    nx.draw_networkx_edges(\n","        final_DAG,\n","        pos,\n","        edge_color=\"white\",\n","        node_shape=\"H\",\n","        node_size=2000,\n","        width=[w + 0.1 for _, _, w, in final_DAG.edges(data=\"weight\")],\n","    )\n","\n","    plt.show()\n","    plt.close()\n","    \n","# data generating functions below this point\n","\n","def simulate_random_dag(d: int,\n","                        degree: float,\n","                        graph_type: str,\n","                        w_range: tuple = (0.5, 2.0)) -> nx.DiGraph:\n","    \"\"\"Simulate random DAG with some expected degree.\n","    Args:\n","        d: number of nodes\n","        degree: expected node degree, in + out\n","        graph_type: {erdos-renyi, barabasi-albert, full}\n","        w_range: weight range +/- (low, high)\n","    Returns:\n","        G: weighted DAG\n","    \"\"\"\n","    if graph_type == 'erdos-renyi':\n","        prob = float(degree) / (d - 1)\n","        B = np.tril((np.random.rand(d, d) < prob).astype(float), k=-1)\n","    elif graph_type == 'barabasi-albert':\n","        m = int(round(degree / 2))\n","        B = np.zeros([d, d])\n","        bag = [0]\n","        for ii in range(1, d):\n","            dest = np.random.choice(bag, size=m)\n","            for jj in dest:\n","                B[ii, jj] = 1\n","            bag.append(ii)\n","            bag.extend(dest)\n","    elif graph_type == 'full':  # ignore degree, only for experimental use\n","        B = np.tril(np.ones([d, d]), k=-1)\n","    else:\n","        raise ValueError('unknown graph type')\n","    # random permutation\n","    P = np.random.permutation(np.eye(d, d))  # permutes first axis only\n","    B_perm = P.T.dot(B).dot(P)\n","    U = np.random.uniform(low=w_range[0], high=w_range[1], size=[d, d])\n","    U[np.random.rand(d, d) < 0.5] *= -1\n","    W = (B_perm != 0).astype(float) * U\n","    G = nx.DiGraph(W)\n","    return G\n","\n","def simulate_sem(G: nx.DiGraph,\n","                 n: int, x_dims: int,\n","                 sem_type: str,\n","                 linear_type: str,\n","                 noise_scale: float = 1.0) -> np.ndarray:\n","    \"\"\"Simulate samples from SEM with specified type of noise.\n","    Args:\n","        G: weigthed DAG\n","        n: number of samples\n","        sem_type: {linear-gauss,linear-exp,linear-gumbel}\n","        noise_scale: scale parameter of noise distribution in linear SEM\n","    Returns:\n","        X: [n,d] sample matrix\n","    \"\"\"\n","    \n","    W = nx.to_numpy_array(G)\n","    d = W.shape[0]\n","    X = np.zeros([n, d, x_dims])\n","    ordered_vertices = list(nx.topological_sort(G))\n","    assert len(ordered_vertices) == d\n","    for j in ordered_vertices:\n","        parents = list(G.predecessors(j))\n","        if linear_type == 'linear':\n","            eta = X[:, parents, 0].dot(W[parents, j])\n","        elif linear_type == 'nonlinear_1':\n","            eta = np.cos(X[:, parents, 0] + 1).dot(W[parents, j])\n","        elif linear_type == 'nonlinear_2':\n","            eta = (X[:, parents, 0]+0.5).dot(W[parents, j])\n","        else:\n","            raise ValueError('unknown linear data type')\n","\n","        if sem_type == 'linear-gauss':\n","            if linear_type == 'linear':\n","                X[:, j, 0] = eta + np.random.normal(scale=noise_scale, size=n)\n","            elif linear_type == 'nonlinear_1':\n","                X[:, j, 0] = eta + np.random.normal(scale=noise_scale, size=n)\n","            elif linear_type == 'nonlinear_2':\n","                X[:, j, 0] = 2.*np.sin(eta) + eta + np.random.normal(scale=noise_scale, size=n)\n","        elif sem_type == 'linear-exp':\n","            X[:, j, 0] = eta + np.random.exponential(scale=noise_scale, size=n)\n","        elif sem_type == 'linear-gumbel':\n","            X[:, j, 0] = eta + np.random.gumbel(scale=noise_scale, size=n)\n","        else:\n","            raise ValueError('unknown sem type')\n","    if x_dims > 1 :\n","        for i in range(x_dims-1):\n","            X[:, :, i+1] = np.random.normal(scale=noise_scale, size=1)*X[:, :, 0] + np.random.normal(scale=noise_scale, size=1) + np.random.normal(scale=noise_scale, size=(n, d))\n","        X[:, :, 0] = np.random.normal(scale=noise_scale, size=1) * X[:, :, 0] + np.random.normal(scale=noise_scale, size=1) + np.random.normal(scale=noise_scale, size=(n, d))\n","    return X\n","\n","def simulate_population_sample(W: np.ndarray,\n","                               Omega: np.ndarray) -> np.ndarray:\n","    \"\"\"Simulate data matrix X that matches population least squares.\n","    Args:\n","        W: [d,d] adjacency matrix\n","        Omega: [d,d] noise covariance matrix\n","    Returns:\n","        X: [d,d] sample matrix\n","    \"\"\"\n","    d = W.shape[0]\n","    X = np.sqrt(d) * slin.sqrtm(Omega).dot(np.linalg.pinv(np.eye(d) - W))\n","    return X\n","\n","def count_accuracy(G_true: nx.DiGraph,\n","                   G: nx.DiGraph,\n","                   G_und: nx.DiGraph = None) -> tuple:\n","    \"\"\"Compute FDR, TPR, and FPR for B, or optionally for CPDAG B + B_und.\n","    Args:\n","        G_true: ground truth graph\n","        G: predicted graph\n","        G_und: predicted undirected edges in CPDAG, asymmetric\n","    Returns:\n","        fdr: (reverse + false positive) / prediction positive\n","        tpr: (true positive) / condition positive\n","        fpr: (reverse + false positive) / condition negative\n","        shd: undirected extra + undirected missing + reverse\n","        nnz: prediction positive\n","    \"\"\"\n","    B_true = nx.to_numpy_array(G_true) != 0\n","    B = nx.to_numpy_array(G) != 0\n","    B_und = None if G_und is None else nx.to_numpy_array(G_und)\n","    d = B.shape[0]\n","    # linear index of nonzeros\n","    if B_und is not None:\n","        pred_und = np.flatnonzero(B_und)\n","    pred = np.flatnonzero(B)\n","    cond = np.flatnonzero(B_true)\n","    cond_reversed = np.flatnonzero(B_true.T)\n","    cond_skeleton = np.concatenate([cond, cond_reversed])\n","    # true pos\n","    true_pos = np.intersect1d(pred, cond, assume_unique=True)\n","    if B_und is not None:\n","        # treat undirected edge favorably\n","        true_pos_und = np.intersect1d(pred_und, cond_skeleton, assume_unique=True)\n","        true_pos = np.concatenate([true_pos, true_pos_und])\n","    # false pos\n","    false_pos = np.setdiff1d(pred, cond_skeleton, assume_unique=True)\n","    if B_und is not None:\n","        false_pos_und = np.setdiff1d(pred_und, cond_skeleton, assume_unique=True)\n","        false_pos = np.concatenate([false_pos, false_pos_und])\n","    # reverse\n","    extra = np.setdiff1d(pred, cond, assume_unique=True)\n","    reverse = np.intersect1d(extra, cond_reversed, assume_unique=True)\n","    # compute ratio\n","    pred_size = len(pred)\n","    if B_und is not None:\n","        pred_size += len(pred_und)\n","    cond_neg_size = 0.5 * d * (d - 1) - len(cond)\n","    fdr = float(len(reverse) + len(false_pos)) / max(pred_size, 1)\n","    tpr = float(len(true_pos)) / max(len(cond), 1)\n","    fpr = float(len(reverse) + len(false_pos)) / max(cond_neg_size, 1)\n","    # structural hamming distance\n","    B_lower = np.tril(B + B.T)\n","    if B_und is not None:\n","        B_lower += np.tril(B_und + B_und.T)\n","    pred_lower = np.flatnonzero(B_lower)\n","    cond_lower = np.flatnonzero(np.tril(B_true + B_true.T))\n","    extra_lower = np.setdiff1d(pred_lower, cond_lower, assume_unique=True)\n","    missing_lower = np.setdiff1d(cond_lower, pred_lower, assume_unique=True)\n","    shd = len(extra_lower) + len(missing_lower) + len(reverse)\n","    return fdr, tpr, fpr, shd, pred_size\n","\n","def count_accuracy_new(G_true: nx.DiGraph,\n","                   G: nx.DiGraph,\n","                   G_und: nx.DiGraph = None) -> tuple:\n","    \"\"\"Compute FDR, TPR, and FPR for B, or optionally for CPDAG B + B_und.\n","    Args:\n","        G_true: ground truth graph\n","        G: predicted graph\n","        G_und: predicted undirected edges in CPDAG, asymmetric\n","    Returns:\n","        fdr: (reverse + false positive) / prediction positive\n","        tpr: (true positive) / condition positive\n","        fpr: (reverse + false positive) / condition negative\n","        shd: undirected extra + undirected missing + reverse\n","        nnz: prediction positive\n","    \"\"\"\n","    B_true = nx.to_numpy_array(G_true) != 0\n","    B = nx.to_numpy_array(G) != 0\n","    B_und = None if G_und is None else nx.to_numpy_array(G_und)\n","    d = B.shape[0]\n","    # linear index of nonzeros\n","    if B_und is not None:\n","        pred_und = np.flatnonzero(B_und)\n","    pred = np.flatnonzero(B)\n","    cond = np.flatnonzero(B_true)\n","    cond_reversed = np.flatnonzero(B_true.T)\n","    cond_skeleton = np.concatenate([cond, cond_reversed])\n","    # true pos\n","    true_pos = np.intersect1d(pred, cond, assume_unique=True)\n","    if B_und is not None:\n","        # treat undirected edge favorably\n","        true_pos_und = np.intersect1d(pred_und, cond_skeleton, assume_unique=True)\n","        true_pos = np.concatenate([true_pos, true_pos_und])\n","    # false pos\n","    false_pos = np.setdiff1d(pred, cond_skeleton, assume_unique=True)\n","    if B_und is not None:\n","        false_pos_und = np.setdiff1d(pred_und, cond_skeleton, assume_unique=True)\n","        false_pos = np.concatenate([false_pos, false_pos_und])\n","    # reverse\n","    extra = np.setdiff1d(pred, cond, assume_unique=True)\n","    reverse = np.intersect1d(extra, cond_reversed, assume_unique=True)\n","    # compute ratio\n","    pred_size = len(pred)\n","    if B_und is not None:\n","        pred_size += len(pred_und)\n","    cond_neg_size = 0.5 * d * (d - 1) - len(cond)\n","    fdr = float(len(reverse) + len(false_pos)) / max(pred_size, 1)\n","    tpr = float(len(true_pos)) / max(len(cond), 1)\n","    fpr = float(len(reverse) + len(false_pos)) / max(cond_neg_size, 1)\n","    # structural hamming distance\n","    B_lower = np.tril(B + B.T)\n","    if B_und is not None:\n","        B_lower += np.tril(B_und + B_und.T)\n","    pred_lower = np.flatnonzero(B_lower)\n","    cond_lower = np.flatnonzero(np.tril(B_true + B_true.T))\n","    extra_lower = np.setdiff1d(pred_lower, cond_lower, assume_unique=True)\n","    missing_lower = np.setdiff1d(cond_lower, pred_lower, assume_unique=True)\n","    shd = len(extra_lower) + len(missing_lower) + len(reverse)\n","    print('extra %f + missing %f + reverse %f' % ( len(extra_lower), len(missing_lower), len(reverse)))\n","\n","    return fdr, tpr, fpr, shd, pred_size, len(extra_lower), len(missing_lower), len(reverse)\n","\n","def compute_BiCScore(G, D):\n","    '''compute the bic score'''\n","    # score = gm.estimators.BicScore(self.data).score(self.model)\n","    origin_score = []\n","    num_var = G.shape[0]\n","    for i in range(num_var):\n","        parents = np.where(G[:,i] !=0)\n","        score_one = compute_local_BiCScore(D, i, parents)\n","        origin_score.append(score_one)\n","\n","    score = sum(origin_score)\n","\n","    return score\n","\n","\n","def compute_local_BiCScore(np_data, target, parents):\n","    # use dictionary\n","    sample_size = np_data.shape[0]\n","    var_size = np_data.shape[1]\n","\n","    # build dictionary and populate\n","    count_d = dict()\n","    if len(parents) < 1:\n","        a = 1\n","\n","    # unique_rows = np.unique(self.np_data, axis=0)\n","    # for data_ind in range(unique_rows.shape[0]):\n","    #     parent_combination = tuple(unique_rows[data_ind,:].reshape(1,-1)[0])\n","    #     count_d[parent_combination] = dict()\n","    #\n","    #     # build children\n","    #     self_value = tuple(self.np_data[data_ind, target].reshape(1,-1)[0])\n","    #     if parent_combination in count_d:\n","    #         if self_value in count_d[parent_combination]:\n","    #             count_d[parent_combination][self_value] += 1.0\n","    #         else:\n","    #             count_d[parent_combination][self_value] = 1.0\n","    #     else:\n","    #         count_d[parent_combination] = dict()\n","    #         count_d\n","\n","    # slower implementation\n","    for data_ind in range(sample_size):\n","        parent_combination = tuple(np_data[data_ind, parents].reshape(1, -1)[0])\n","        self_value = tuple(np_data[data_ind, target].reshape(1, -1)[0])\n","        if parent_combination in count_d:\n","            if self_value in count_d[parent_combination]:\n","                count_d[parent_combination][self_value] += 1.0\n","            else:\n","                count_d[parent_combination][self_value] = 1.0\n","        else:\n","            count_d[parent_combination] = dict()\n","            count_d[parent_combination][self_value] = 1.0\n","\n","    # compute likelihood\n","    loglik = 0.0\n","    # for data_ind in range(sample_size):\n","    # if len(parents) > 0:\n","    num_parent_state = np.prod(np.amax(np_data[:, parents], axis=0) + 1)\n","    # else:\n","    #    num_parent_state = 0\n","    num_self_state = np.amax(np_data[:, target], axis=0) + 1\n","\n","    for parents_state in count_d:\n","        local_count = sum(count_d[parents_state].values())\n","        for self_state in count_d[parents_state]:\n","            loglik += count_d[parents_state][self_state] * (\n","                        math.log(count_d[parents_state][self_state] + 0.1) - math.log(local_count))\n","\n","    # penality\n","    num_param = num_parent_state * (\n","                num_self_state - 1)  # count_faster(count_d) - len(count_d) - 1 # minus top level and minus one\n","    bic = loglik - 0.5 * math.log(sample_size) * num_param\n","\n","    return bic    \n","\n","def data_to_tensor_dataset(X, batch_size, G=None):\n","        \n","    feat_train = torch.FloatTensor(X)\n","    train_data = TensorDataset(feat_train, feat_train)\n","    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n","    \n","    return train_data_loader, G\n","\n","def load_data(args, batch_size=1000, suffix='', debug = False):\n","    #  # configurations\n","    n, d = args.data_sample_size, args.data_variable_size\n","    graph_type, degree, sem_type, linear_type = args.graph_type, args.graph_degree, args.graph_sem_type, args.graph_linear_type\n","    x_dims = args.x_dims\n","\n","    if args.data_type == 'synthetic':\n","        # generate data\n","        G = simulate_random_dag(d, degree, graph_type)\n","        X = simulate_sem(G, n, x_dims, sem_type, linear_type)\n","        \n","        train_data_loader, G = data_to_tensor_dataset(X, batch_size, G)\n","        return train_data_loader, G\n","\n","    elif args.data_type == 'real':\n","        #this where you can use your own dataset\n","        assert args.path != '', 'Data path must be specified'\n","        fdpp = FullDataPreProcessor(args.path, args.column_names_list, args.initial_identifier, args.num_of_rows, args.seed)\n","        preprocessed_dataframe = fdpp.get_dataframe()\n","        columns = fdpp.sample_dataframe(preprocessed_dataframe[0]).columns\n","        X = fdpp.sample_dataframe(preprocessed_dataframe[0]).values\n","                \n","        train_data_loader, G = data_to_tensor_dataset(X, batch_size)\n","        return train_data_loader, X.shape[1], columns\n","    \n","    elif args.data_type == 'benchmark':\n","        # create your own version of benchmark discrete data\n","        assert args.path != '', 'Data path must be specified'\n","        file_path_dataset = os.path.join(args.path, 'pathfinder_5000.txt') #e.g for pathfinder benchmark dataset it should be something like pathfinder_5000.txt\n","        \n","        # read file\n","        data = np.loadtxt(file_path_dataset, skiprows =0, dtype=np.int32)\n","        \n","        #find how many categories there are\n","        num_cats = num_categories(data.flatten())\n","            \n","        # read ground truth graph\n","        file_path = os.path.join(args.path, 'pathfinder_graph.txt') #e.g for pathfinder benchmark dataset it should be somethiing like pathfinder_graph.txt\n","        \n","        graph = np.loadtxt(file_path, skiprows =0, dtype=np.int32)\n","            \n","        G = nx.DiGraph(graph)\n","        X = data[:args.num_of_rows]\n","        \n","        train_data_loader, G = data_to_tensor_dataset(X, batch_size, G)\n","\n","        return train_data_loader, X.shape[1], G, num_cats\n","    \n","  "],"metadata":{"id":"Y9oEpYOonyC5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## FullDataPreProcessor.py"],"metadata":{"id":"7OjxvNInnykm"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Mon Nov  2 20:13:45 2020\n","@inproceedings{xu2019modeling,\n","  title={Modeling Tabular data using Conditional GAN},\n","  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},\n","  booktitle={Advances in Neural Information Processing Systems},\n","  year={2019}\n","}\n","@article{torfi2020cor,\n","title={COR-GAN: Correlation-Capturing Convolutional Neural Networks for Generating Synthetic Healthcare Records},\n","author={Torfi, Amirsina and Fox, Edward A},\n","journal={arXiv preprint arXiv:2001.09346},\n","year={2020}\n","}\n","\"\"\"\n","\n","#Importing libraries and frameworks\n","import os\n","import numpy as np\n","import torch\n","import pandas as pd\n","# from ctgan.data import read_csv\n","from pandas import read_csv\n","\n","class FullDataPreProcessor:\n","    \n","    def __init__(self, path, column_names, initial_identifier, num_of_rows, seed):\n","        self.path = path\n","        self.column_names = column_names\n","        self.initial_identifier = initial_identifier\n","        self.num_of_rows = num_of_rows\n","        self.seed = seed\n","    \n","    def get_dataframe(self):\n","        \n","        df = read_csv(self.path)\n","        \n","        #Getting all of the columns with regards to their dtype\n","        non_numeric_columns = list(df[0].select_dtypes(exclude=['int64','float64']).columns)\n","        numeric_int_columns = list(df[0].select_dtypes(include=['int64']).columns)\n","        numeric_float_columns = list(df[0].select_dtypes(include=['float64']).columns)\n","            \n","        #Filling in all of the missing data of type string\n","        for j in range(len(non_numeric_columns)):\n","            df[0][non_numeric_columns[j]].fillna('emptyblock', inplace = True)\n","            \n","        #Filling in all of the missing data of type int\n","        for k in range(len(numeric_int_columns)):\n","            df[0][numeric_int_columns[k]].fillna(-123456789, inplace = True)\n","             \n","        #Filling in all of the missing data of type float    \n","        for l in range(len(numeric_float_columns)):\n","            df[0][numeric_float_columns[l]].fillna(-1234.56789, inplace = True)\n","        \n","        return df\n","    \n","    def sample_dataframe(self, dataframe): \n","        if self.column_names != []:\n","            dataframes = []\n","            #assert self.initial_identifier != '', 'Initial Identifier not specified! Choose one of the following: ' + str(list(dataframe.columns))\n","            #initial_df = pd.DataFrame({self.initial_identifier: dataframe[self.initial_identifier]})\n","            #dataframes.append(initial_df)\n","            for column in self.column_names:\n","                tmpdf = pd.DataFrame({column: dataframe[column]})\n","                dataframes.append(tmpdf)\n","            new_df = pd.concat(dataframes, axis=1)\n","            if self.num_of_rows != -1:\n","                assert self.num_of_rows > 0, 'Number of rows must be greater than zero'\n","                assert self.num_of_rows <= dataframe.shape[0], 'Number of rows must less or equal to the total number of rows'\n","                sampled_df = new_df.sample(self.num_of_rows, random_state=self.seed)\n","                sampled_df.sort_index(inplace=True)\n","                return sampled_df\n","            else:\n","                return new_df\n","        else:\n","            return dataframe\n","    \n","class Dataset:\n","    def __init__(self, data, transform=None):\n","\n","        # Transform\n","        self.transform = transform\n","\n","        # load data here\n","        self.data = data\n","        self.sampleSize = data.shape[0]\n","        self.featureSize = data.shape[1]\n","\n","    def return_data(self):\n","        return self.data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        sample = self.data[idx]\n","        sample = np.clip(sample, 0, 1)\n","\n","        if self.transform:\n","           pass\n","\n","        return torch.from_numpy(sample)\n"],"metadata":{"id":"c-HxZpvBn1JU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RDP_accountant.py"],"metadata":{"id":"-yOnkmomn1ai"}},{"cell_type":"code","source":["# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\"\"\"RDP analysis of the Sampled Gaussian Mechanism.\n","Functionality for computing Renyi differential privacy (RDP) of an additive\n","Sampled Gaussian Mechanism (SGM). Its public interface consists of two methods:\n","  compute_rdp(q, noise_multiplier, T, orders) computes RDP for SGM iterated\n","                                   T times.\n","  get_privacy_spent(orders, rdp, target_eps, target_delta) computes delta\n","                                   (or eps) given RDP at multiple orders and\n","                                   a target value for eps (or delta).\n","Example use:\n","Suppose that we have run an SGM applied to a function with l2-sensitivity 1.\n","Its parameters are given as a list of tuples (q1, sigma1, T1), ...,\n","(qk, sigma_k, Tk), and we wish to compute eps for a given delta.\n","The example code would be:\n","  max_order = 32\n","  orders = range(2, max_order + 1)\n","  rdp = np.zeros_like(orders, dtype=float)\n","  for q, sigma, T in parameters:\n","   rdp += rdp_accountant.compute_rdp(q, sigma, T, orders)\n","  eps, _, opt_order = rdp_accountant.get_privacy_spent(rdp, target_delta=delta)\n","\"\"\"\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import math\n","import sys\n","\n","import numpy as np\n","from scipy import special\n","import six\n","\n","########################\n","# LOG-SPACE ARITHMETIC #\n","########################\n","\n","\n","def _log_add(logx, logy):\n","  \"\"\"Add two numbers in the log space.\"\"\"\n","  a, b = min(logx, logy), max(logx, logy)\n","  if a == -np.inf:  # adding 0\n","    return b\n","  # Use exp(a) + exp(b) = (exp(a - b) + 1) * exp(b)\n","  return math.log1p(math.exp(a - b)) + b  # log1p(x) = log(x + 1)\n","\n","\n","def _log_sub(logx, logy):\n","  \"\"\"Subtract two numbers in the log space. Answer must be non-negative.\"\"\"\n","  if logx < logy:\n","    raise ValueError(\"The result of subtraction must be non-negative.\")\n","  if logy == -np.inf:  # subtracting 0\n","    return logx\n","  if logx == logy:\n","    return -np.inf  # 0 is represented as -np.inf in the log space.\n","\n","  try:\n","    # Use exp(x) - exp(y) = (exp(x - y) - 1) * exp(y).\n","    return math.log(math.expm1(logx - logy)) + logy  # expm1(x) = exp(x) - 1\n","  except OverflowError:\n","    return logx\n","\n","\n","def _log_print(logx):\n","  \"\"\"Pretty print.\"\"\"\n","  if logx < math.log(sys.float_info.max):\n","    return \"{}\".format(math.exp(logx))\n","  else:\n","    return \"exp({})\".format(logx)\n","\n","\n","def _compute_log_a_int(q, sigma, alpha):\n","  \"\"\"Compute log(A_alpha) for integer alpha. 0 < q < 1.\"\"\"\n","  assert isinstance(alpha, six.integer_types)\n","\n","  # Initialize with 0 in the log space.\n","  log_a = -np.inf\n","\n","  for i in range(alpha + 1):\n","    log_coef_i = (\n","        math.log(special.binom(alpha, i)) + i * math.log(q) +\n","        (alpha - i) * math.log(1 - q))\n","\n","    s = log_coef_i + (i * i - i) / (2 * (sigma**2))\n","    log_a = _log_add(log_a, s)\n","\n","  return float(log_a)\n","\n","\n","def _compute_log_a_frac(q, sigma, alpha):\n","  \"\"\"Compute log(A_alpha) for fractional alpha. 0 < q < 1.\"\"\"\n","  # The two parts of A_alpha, integrals over (-inf,z0] and [z0, +inf), are\n","  # initialized to 0 in the log space:\n","  log_a0, log_a1 = -np.inf, -np.inf\n","  i = 0\n","\n","  z0 = sigma**2 * math.log(1 / q - 1) + .5\n","\n","  while True:  # do ... until loop\n","    coef = special.binom(alpha, i)\n","    log_coef = math.log(abs(coef))\n","    j = alpha - i\n","\n","    log_t0 = log_coef + i * math.log(q) + j * math.log(1 - q)\n","    log_t1 = log_coef + j * math.log(q) + i * math.log(1 - q)\n","\n","    log_e0 = math.log(.5) + _log_erfc((i - z0) / (math.sqrt(2) * sigma))\n","    log_e1 = math.log(.5) + _log_erfc((z0 - j) / (math.sqrt(2) * sigma))\n","\n","    log_s0 = log_t0 + (i * i - i) / (2 * (sigma**2)) + log_e0\n","    log_s1 = log_t1 + (j * j - j) / (2 * (sigma**2)) + log_e1\n","\n","    if coef > 0:\n","      log_a0 = _log_add(log_a0, log_s0)\n","      log_a1 = _log_add(log_a1, log_s1)\n","    else:\n","      log_a0 = _log_sub(log_a0, log_s0)\n","      log_a1 = _log_sub(log_a1, log_s1)\n","\n","    i += 1\n","    if max(log_s0, log_s1) < -30:\n","      break\n","\n","  return _log_add(log_a0, log_a1)\n","\n","\n","def _compute_log_a(q, sigma, alpha):\n","  \"\"\"Compute log(A_alpha) for any positive finite alpha.\"\"\"\n","  if float(alpha).is_integer():\n","    return _compute_log_a_int(q, sigma, int(alpha))\n","  else:\n","    return _compute_log_a_frac(q, sigma, alpha)\n","\n","\n","def _log_erfc(x):\n","  \"\"\"Compute log(erfc(x)) with high accuracy for large x.\"\"\"\n","  try:\n","    return math.log(2) + special.log_ndtr(-x * 2**.5)\n","  except NameError:\n","    # If log_ndtr is not available, approximate as follows:\n","    r = special.erfc(x)\n","    if r == 0.0:\n","      # Using the Laurent series at infinity for the tail of the erfc function:\n","      #     erfc(x) ~ exp(-x^2-.5/x^2+.625/x^4)/(x*pi^.5)\n","      # To verify in Mathematica:\n","      #     Series[Log[Erfc[x]] + Log[x] + Log[Pi]/2 + x^2, {x, Infinity, 6}]\n","      return (-math.log(math.pi) / 2 - math.log(x) - x**2 - .5 * x**-2 +\n","              .625 * x**-4 - 37. / 24. * x**-6 + 353. / 64. * x**-8)\n","    else:\n","      return math.log(r)\n","\n","def _compute_delta(orders, rdp, eps):\n","  \"\"\"Compute delta given a list of RDP values and target epsilon.\n","  Args:\n","    orders: An array (or a scalar) of orders.\n","    rdp: A list (or a scalar) of RDP guarantees.\n","    eps: The target epsilon.\n","  Returns:\n","    Pair of (delta, optimal_order).\n","  Raises:\n","    ValueError: If input is malformed.\n","  \"\"\"\n","  orders_vec = np.atleast_1d(orders)\n","  rdp_vec = np.atleast_1d(rdp)\n","\n","  if len(orders_vec) != len(rdp_vec):\n","    raise ValueError(\"Input lists must have the same length.\")\n","\n","  deltas = np.exp((rdp_vec - eps) * (orders_vec - 1))\n","  idx_opt = np.argmin(deltas)\n","  return min(deltas[idx_opt], 1.), orders_vec[idx_opt]\n","\n","\n","def _compute_eps(orders, rdp, delta):\n","  \"\"\"Compute epsilon given a list of RDP values and target delta.\n","  Args:\n","    orders: An array (or a scalar) of orders.\n","    rdp: A list (or a scalar) of RDP guarantees.\n","    delta: The target delta.\n","  Returns:\n","    Pair of (eps, optimal_order).\n","  Raises:\n","    ValueError: If input is malformed.\n","  \"\"\"\n","  orders_vec = np.atleast_1d(orders)\n","  rdp_vec = np.atleast_1d(rdp)\n","\n","  if len(orders_vec) != len(rdp_vec):\n","    raise ValueError(\"Input lists must have the same length.\")\n","\n","  eps = rdp_vec - math.log(delta) / (orders_vec - 1)\n","\n","  idx_opt = np.nanargmin(eps)  # Ignore NaNs\n","  return eps[idx_opt], orders_vec[idx_opt]\n","\n","\n","def _compute_rdp(q, sigma, alpha):\n","  \"\"\"Compute RDP of the Sampled Gaussian mechanism at order alpha.\n","  Args:\n","    q: The sampling rate.\n","    sigma: The std of the additive Gaussian noise.\n","    alpha: The order at which RDP is computed.\n","  Returns:\n","    RDP at alpha, can be np.inf.\n","  \"\"\"\n","  if q == 0:\n","    return 0\n","\n","  if q == 1.:\n","    return alpha / (2 * sigma**2)\n","\n","  if np.isinf(alpha):\n","    return np.inf\n","\n","  return _compute_log_a(q, sigma, alpha) / (alpha - 1)\n","\n","\n","def compute_rdp(q, noise_multiplier, steps, orders):\n","  \"\"\"Compute RDP of the Sampled Gaussian Mechanism.\n","  Args:\n","    q: The sampling rate.\n","    noise_multiplier: The ratio of the standard deviation of the Gaussian noise\n","        to the l2-sensitivity of the function to which it is added.\n","    steps: The number of steps.\n","    orders: An array (or a scalar) of RDP orders.\n","  Returns:\n","    The RDPs at all orders, can be np.inf.\n","  \"\"\"\n","  if np.isscalar(orders):\n","    rdp = _compute_rdp(q, noise_multiplier, orders)\n","  else:\n","    rdp = np.array([_compute_rdp(q, noise_multiplier, order)\n","                    for order in orders])\n","\n","  return rdp * steps\n","\n","\n","def get_privacy_spent(orders, rdp, target_eps=None, target_delta=None):\n","  \"\"\"Compute delta (or eps) for given eps (or delta) from RDP values.\n","  Args:\n","    orders: An array (or a scalar) of RDP orders.\n","    rdp: An array of RDP values. Must be of the same length as the orders list.\n","    target_eps: If not None, the epsilon for which we compute the corresponding\n","              delta.\n","    target_delta: If not None, the delta for which we compute the corresponding\n","              epsilon. Exactly one of target_eps and target_delta must be None.\n","  Returns:\n","    eps, delta, opt_order.\n","  Raises:\n","    ValueError: If target_eps and target_delta are messed up.\n","  \"\"\"\n","  if target_eps is None and target_delta is None:\n","    raise ValueError(\n","        \"Exactly one out of eps and delta must be None. (Both are).\")\n","\n","  if target_eps is not None and target_delta is not None:\n","    raise ValueError(\n","        \"Exactly one out of eps and delta must be None. (None is).\")\n","\n","  if target_eps is not None:\n","    delta, opt_order = _compute_delta(orders, rdp, target_eps)\n","    return target_eps, delta, opt_order\n","  else:\n","    eps, opt_order = _compute_eps(orders, rdp, target_delta)\n","    return eps, target_delta, opt_order\n","\n","\n","def compute_rdp_from_ledger(ledger, orders):\n","  \"\"\"Compute RDP of Sampled Gaussian Mechanism from ledger.\n","  Args:\n","    ledger: A formatted privacy ledger.\n","    orders: An array (or a scalar) of RDP orders.\n","  Returns:\n","    RDP at all orders, can be np.inf.\n","  \"\"\"\n","  total_rdp = np.zeros_like(orders, dtype=float)\n","  for sample in ledger:\n","    # Compute equivalent z from l2_clip_bounds and noise stddevs in sample.\n","    # See https://arxiv.org/pdf/1812.06210.pdf for derivation of this formula.\n","    effective_z = sum([\n","        (q.noise_stddev / q.l2_norm_bound)**-2 for q in sample.queries])**-0.5\n","    total_rdp += compute_rdp(\n","        sample.selection_probability, effective_z, 1, orders)\n","  return total_rdp\n"],"metadata":{"id":"mUgRUtDdn4Yr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Private_AAE_WGAN_GP.py"],"metadata":{"id":"H-tT8iDmn6g6"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Mon May 10 18:37:53 2021\n","@author: Hristo Petkov\n","\"\"\"\n","\n","\"\"\"\n","@inproceedings{yu2019dag,\n","  title={DAG-GNN: DAG Structure Learning with Graph Neural Networks},\n","  author={Yue Yu, Jie Chen, Tian Gao, and Mo Yu},\n","  booktitle={Proceedings of the 36th International Conference on Machine Learning},\n","  year={2019}\n","}\n","@inproceedings{xu2019modeling,\n","  title={Modeling Tabular data using Conditional GAN},\n","  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},\n","  booktitle={Advances in Neural Information Processing Systems},\n","  year={2019}\n","}\n","\"\"\"\n","\n","import time\n","import torch\n","import math\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import networkx as nx\n","import scipy.linalg as slin\n","import os\n","\n","from torch.autograd import Variable\n","from torch import optim\n","from torch.optim import lr_scheduler\n","from torch.nn import Linear, Sequential, LeakyReLU, Dropout, BatchNorm1d\n","# from Utils import preprocess_adj_new, preprocess_adj_new1\n","# from Utils import nll_gaussian, kl_gaussian_sem,  nll_catogrical\n","# from Utils import _h_A\n","# from Utils import count_accuracy  \n","    \n","class Discriminator(nn.Module):\n","    \"\"\"Discriminator module.\"\"\"\n","    def __init__(self, input_dim, discriminator_dim, negative_slope, dropout_rate, pac=10):\n","        super(Discriminator, self).__init__()\n","        dim = input_dim * pac\n","        self.pac = pac\n","        self.pacdim = dim\n","        \n","        seq = []\n","        for item in list(discriminator_dim):\n","            seq += [Linear(dim, item), LeakyReLU(negative_slope), Dropout(dropout_rate)]\n","            dim = item\n","\n","        seq += [Linear(dim, 1)]\n","        self.seq = Sequential(*seq)\n","        self.init_weights()\n","        \n","    def init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, Linear):\n","                nn.init.xavier_normal_(m.weight.data)\n","                m.bias.data.fill_(0.0)\n","            elif isinstance(m, BatchNorm1d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def calc_gradient_penalty(self, real_data, fake_data, data_type, device='cpu', pac=10, lambda_=10):\n","        \n","        # reshape data\n","        real_data = real_data.squeeze()\n","        fake_data = fake_data.squeeze()\n","        \n","        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n","        alpha = alpha.repeat(1, pac, real_data.size(1))\n","        alpha = alpha.view(-1, real_data.size(1))\n","        \n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","        disc_interpolates = self(interpolates)\n","\n","        gradients = torch.autograd.grad(\n","            outputs=disc_interpolates, inputs=interpolates,\n","            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n","            create_graph=True, retain_graph=True, only_inputs=True\n","        )[0]\n","\n","        gradient_penalty = ((\n","            gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n","        ) ** 2).mean() * lambda_\n","\n","        return gradient_penalty\n","\n","    def forward(self, input):\n","        assert input.size()[0] % self.pac == 0\n","        return self.seq(input.view(-1, self.pacdim))\n","    \n","class Generator(nn.Module):\n","    \"\"\"Generator module (based on DAG-NotearsMLP)\"\"\"\n","    def __init__(self, m, dims, bias=True):\n","        super(Generator, self).__init__()\n","        assert len(dims) >= 2\n","        assert dims[-1] == 1\n","        d = dims[0]\n","        self.dims = dims\n","        # fc1: variable splitting for l1\n","        self.fc1_pos = nn.Linear(d, d * dims[1], bias=bias)\n","        self.fc1_neg = nn.Linear(d, d * dims[1], bias=bias)\n","        self.fc1_pos.weight.bounds = self._bounds()\n","        self.fc1_neg.weight.bounds = self._bounds()\n","        # fc2: local linear layers\n","        layers = []\n","        for l in range(len(dims) - 2):\n","            layers.append(LocallyConnected(d, dims[l + 1]+m, dims[l + 2], bias=bias))\n","        self.fc2 = nn.ModuleList(layers)\n","        self.init_weights()\n","        \n","    def init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, Linear):\n","                nn.init.xavier_normal_(m.weight.data)\n","                m.bias.data.fill_(0.0)\n","            elif isinstance(m, BatchNorm1d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _bounds(self):\n","        d = self.dims[0]\n","        bounds = []\n","        for j in range(d):\n","            for m in range(self.dims[1]):\n","                for i in range(d):\n","                    if i == j:\n","                        bound = (0, 0)\n","                    else:\n","                        bound = (0, None)\n","                    bounds.append(bound)\n","        return bounds\n","\n","    def forward(self, x, n, d, m):  # [n, d] -> [n, d]\n","        x = self.fc1_pos(x) - self.fc1_neg(x)  # [n, d * m1]\n","        x = x.view(-1, self.dims[0], self.dims[1])  # [n, d, m1]\n","        for fc in self.fc2:\n","            # x = F.elu(x)  # [n, d, m1]\n","            x = torch.sigmoid(x)  # [n, d, m1]\n","            z = Variable(torch.FloatTensor(np.random.normal(0, 1, (x.size(0), d, m)))).double().cuda()\n","            x = torch.cat((x,z), dim=2)\n","            x = fc(x)  # [n, d, m2]\n","        x = x.squeeze(dim=2)  # [n, d]\n","        return x\n","\n","    def h_func(self):\n","        \"\"\"Constrain 2-norm-squared of fc1 weights along m1 dim to be a DAG\"\"\"\n","        d = self.dims[0]\n","        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n","        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n","        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n","        h = trace_expm(A) - d  # (Zheng et al. 2018)\n","        # A different formulation, slightly faster at the cost of numerical stability\n","        # M = torch.eye(d) + A / d  # (Yu et al. 2019)\n","        # E = torch.matrix_power(M, d - 1)\n","        # h = (E.t() * M).sum() - d\n","        return h\n","\n","    def l2_reg(self):\n","        \"\"\"Take 2-norm-squared of all parameters\"\"\"\n","        reg = 0.\n","        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n","        reg += torch.sum(fc1_weight ** 2)\n","        for fc in self.fc2:\n","            reg += torch.sum(fc.weight ** 2)\n","        return reg\n","\n","    def fc1_l1_reg(self):\n","        \"\"\"Take l1 norm of fc1 weight\"\"\"\n","        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n","        return reg\n","\n","    @torch.no_grad()\n","    def fc1_to_adj(self) -> np.ndarray:  # [j * m1, i] -> [i, j]\n","        \"\"\"Get W from fc1 weights, take 2-norm over m1 dim\"\"\"\n","        d = self.dims[0]\n","        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n","        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n","        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n","        W = torch.sqrt(A)  # [i, j]\n","        W = W.cpu().detach().numpy()  # [i, j]\n","        return W\n","    \n","class LocallyConnected(nn.Module):\n","    \"\"\"Local linear layer, i.e. Conv1dLocal() with filter size 1.\n","    Args:\n","        num_linear: num of local linear layers, i.e.\n","        in_features: m1\n","        out_features: m2\n","        bias: whether to include bias or not\n","    Shape:\n","        - Input: [n, d, m1]\n","        - Output: [n, d, m2]\n","    Attributes:\n","        weight: [d, m1, m2]\n","        bias: [d, m2]\n","    \"\"\"\n","\n","    def __init__(self, num_linear, input_features, output_features, bias=True):\n","        super(LocallyConnected, self).__init__()\n","        self.num_linear = num_linear\n","        self.input_features = input_features\n","        self.output_features = output_features\n","\n","        self.weight = nn.Parameter(torch.Tensor(num_linear,\n","                                                input_features,\n","                                                output_features))\n","        if bias:\n","            self.bias = nn.Parameter(torch.Tensor(num_linear, output_features))\n","        else:\n","            # You should always register all possible parameters, but the\n","            # optional ones can be None if you want.\n","            self.register_parameter('bias', None)\n","\n","        self.reset_parameters()\n","\n","    @torch.no_grad()\n","    def reset_parameters(self):\n","        k = 1.0 / self.input_features\n","        bound = math.sqrt(k)\n","        nn.init.uniform_(self.weight, -bound, bound)\n","        if self.bias is not None:\n","            nn.init.uniform_(self.bias, -bound, bound)\n","\n","    def forward(self, input: torch.Tensor):\n","        # [n, d, 1, m2] = [n, d, 1, m1] @ [1, d, m1, m2]\n","        out = torch.matmul(input.unsqueeze(dim=2), self.weight.unsqueeze(dim=0))\n","        out = out.squeeze(dim=2)\n","        if self.bias is not None:\n","            # [n, d, m2] += [d, m2]\n","            out += self.bias\n","        return out\n","\n","    def extra_repr(self):\n","        # (Optional)Set the extra information about this module. You can test\n","        # it by printing an object of this class.\n","        return 'num_linear={}, in_features={}, out_features={}, bias={}'.format(\n","            self.num_linear, self.in_features, self.out_features,\n","            self.bias is not None\n","        )\n","\n","class TraceExpm(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input):\n","        # detach so we can cast to NumPy\n","        E = slin.expm(input.detach().cpu().numpy())\n","        f = np.trace(E)\n","        E = torch.from_numpy(E)\n","        ctx.save_for_backward(E)\n","        return torch.as_tensor(f, dtype=input.dtype)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        E, = ctx.saved_tensors\n","        grad_input = grad_output * E.t()\n","        return grad_input.cuda()\n","\n","trace_expm = TraceExpm.apply\n","    \n","class AAE_WGAN_GP(nn.Module):\n","    \"\"\"DAG-AAE model/framework.\"\"\"\n","    def __init__(self, args, adj_A):\n","        super(AAE_WGAN_GP, self).__init__()\n","        \n","        self.data_type = args.data_type\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.batch_size = args.batch_size\n","        \n","        self.discriminator_steps = args.discriminator_steps\n","        self.epochs = args.epochs\n","        self.lr = args.lr\n","        \n","        self.c_A = args.c_A\n","        self.lambda_A = args.lambda_A\n","        self.tau_A = args.tau_A\n","        self.graph_threshold = args.graph_threshold\n","        \n","        self.x_dims = args.x_dims\n","        self.z_dims = args.z_dims\n","        self.encoder_hidden = args.encoder_hidden\n","        self.decoder_hidden = args.decoder_hidden\n","        self.adj_A = adj_A\n","        \n","        self.k_max_iter = int(args.k_max_iter)\n","        self.h_tol = args.h_tol\n","        \n","        self.h_A_new = torch.tensor(1.)\n","        self.h_A_old = np.inf\n","        \n","        self.discrete_columns = args.discrete_column_names_list\n","        self.data_variable_size = self.adj_A.shape[1]\n","        \n","        self.mul1 = args.mul1\n","        self.mul2 = args.mul2\n","        \n","        self.lr_decay = args.lr_decay\n","        self.gamma = args.gamma\n","        self.negative_slope = args.negative_slope\n","        self.dropout_rate = args.dropout_rate\n","        \n","        self.differentialPrivacy = args.differentialPrivacy\n","        self.EPSILON = args.EPSILON\n","        self.DELTA = args.DELTA\n","        self.MAX_GRAD_NORM = args.MAX_GRAD_NORM\n","        self.SIGMA = args.SIGMA\n","        # usually, we should calculate SIGMA. But, to get things moving, lets assume values from literature: DP-GAN uses SIGMA=2\n","        self.MICRO_BATCH_SIZE = args.MICRO_BATCH_SIZE\n","        # any batch size that we use needs to be compatible with Hristo's model: he needs batch size to be divisible by number of variables.\n","        # micro batch size should thus be set accordingly. For now, assume = 10 (since his batch size is 100)\n","        self.priv_steps = 0\n","        self.eps = 0\n","\n","    def forward(self, inputs):\n","        fake_data = self.generator(inputs.squeeze(), self.batch_size, self.data_variable_size, self.z_dims)\n","        return fake_data\n","                \n","    def update_optimizer(self, optimizer, original_lr, c_A):\n","        '''related LR to c_A, whenever c_A gets big, reduce LR proportionally'''\n","        MAX_LR = 1e-2\n","        MIN_LR = 1e-4\n","\n","        estimated_lr = original_lr / (math.log10(c_A) + 1e-10)\n","        if estimated_lr > MAX_LR:\n","            lr = MAX_LR\n","        elif estimated_lr < MIN_LR:\n","            lr = MIN_LR\n","        else:\n","            lr = estimated_lr\n","\n","        # set LR\n","        for parame_group in optimizer.param_groups:\n","            parame_group['lr'] = lr\n","\n","        return optimizer, lr\n","    \n","    def train(self, train_loader, epoch, best_val_loss, ground_truth_G, lambda_A, c_A, optimizerG, optimizerD):\n","        '''training algorithm for a single epoch'''\n","        t = time.time()\n","        nll_train = []\n","        kl_train = []\n","        mse_train = []\n","        shd_trian = []\n","\n","        # self.schedulerG.step()\n","        # self.schedulerD.step()\n","\n","        # update optimizer\n","        optimizerG, lr = self.update_optimizer(optimizerG, self.lr, c_A)\n","        optimizerD, lr = self.update_optimizer(optimizerD, self.lr, c_A)\n","\n","        for batch_idx, (data, relations) in enumerate(train_loader):\n","            for n in range(self.discriminator_steps):\n","                ###################################################################\n","                # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n","                ###################################################################\n","                \n","                data, relations = Variable(data.to(self.device)).double(), Variable(relations.to(self.device)).double()\n","                \n","                if self.data_type != 'synthetic':\n","                    data = data.unsqueeze(2)\n","                \n","                optimizerD.zero_grad()\n","                \n","                if self.differentialPrivacy:\n","                      \n","                    clipped_grads = {\n","                        name: torch.zeros_like(param) for name, param in self.discriminator.named_parameters()}\n","                    \n","                    for k in range(int(data.size(0) / self.MICRO_BATCH_SIZE)): # for each micro-batch, \n","                        # truncate data into MICROBATCH\n","                        data_micro = data[k * self.MICRO_BATCH_SIZE: (k + 1) * self.MICRO_BATCH_SIZE] # average the fake output\n","                        # pass MICROBATCH through generator: making sure data.size(0) evenly divisible by PAC (=10)\n","                        fake_data = self(data_micro)\n","                        # pass MICROBATCH through discriminator \n","                        y_fake = self.discriminator(fake_data) \n","                        y_real = self.discriminator(data_micro)     \n","                        if self.x_dims > 1:\n","                            #vector case\n","                            pen = self.discriminator.calc_gradient_penalty(\n","                                data_micro.view(-1, data_micro.size(1) * data_micro.size(2)), fake_data.view(-1, fake_data.size(1) * fake_data.size(2)), self.data_type, self.device) \n","                            loss_d = -(torch.mean(F.softplus(y_real)) - torch.mean(F.softplus(y_fake)))\n","                        else:\n","                            #normal continious and discrete data case\n","                            pen = self.discriminator.calc_gradient_penalty(\n","                                    data_micro, fake_data, self.data_type, self.device) \n","                            loss_d = -(torch.mean(F.softplus(y_real)) - torch.mean(F.softplus(y_fake)))\n","                  \n","                        # accumulate gradients \n","                        pen.backward(retain_graph=True)\n","                        loss_d.backward()     \n","                        # now clip them\n","                        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), self.MAX_GRAD_NORM)\n","                        for name, param in self.discriminator.named_parameters():\n","                            clipped_grads[name] += param.grad\n","                        # grads saved in clipped_grads, so remove from params\n","                        self.discriminator.zero_grad()\n","\n","                    # last step: now attach the clipped gradients and add noise to them\n","                    for name, param in self.discriminator.named_parameters():\n","                        param.grad = (clipped_grads[name] + torch.FloatTensor(\n","                            clipped_grads[name].size()).normal_(0, self.SIGMA * self.MAX_GRAD_NORM).to(self.device)) / (\n","                                                  data.size(0) / self.MICRO_BATCH_SIZE)\n","                   \n","                    # update parameters with privatised (bounded) gradients \n","                    optimizerD.step() \n","\n","                    # increment counter for the #times we update based on privatised grads\n","                    self.priv_steps += 1\n","\n","                else:\n","\n","                    fake_data = self(data)\n","                    y_fake = self.discriminator(fake_data)\n","                    y_real = self.discriminator(data)\n","                \n","                    if self.x_dims > 1:\n","                        #vector case\n","                        pen = self.discriminator.calc_gradient_penalty(\n","                            data.view(-1, data.size(1) * data.size(2)), fake_data.view(-1, fake_data.size(1) * fake_data.size(2)), self.data_type, self.device) \n","                        loss_d = -(torch.mean(F.softplus(y_real)) - torch.mean(F.softplus(y_fake)))\n","                    else:\n","                        #normal continious and discrete data case\n","                        pen = self.discriminator.calc_gradient_penalty(\n","                                data, fake_data, self.data_type, self.device) \n","                        loss_d = -(torch.mean(F.softplus(y_real)) - torch.mean(F.softplus(y_fake)))\n","                        \n","                    pen.backward(retain_graph=True)\n","                    loss_d.backward()\n","                    loss_d = optimizerD.step() \n","\n","            \n","            ###############################################\n","            # (2) Update G network: maximize log(D(G(z)))\n","            ###############################################\n","\n","            optimizerG.zero_grad()\n","\n","            if self.differentialPrivacy:\n","                      \n","                clipped_gen_grads = {\n","                    name: torch.zeros_like(param) for name, param in self.generator.named_parameters()}\n","                  \n","                for k in range(int(data.size(0) / self.MICRO_BATCH_SIZE)): # for each micro-batch,\n","                    # truncate data into MICROBATCH\n","                    data_micro = data[k * self.MICRO_BATCH_SIZE: (k + 1) * self.MICRO_BATCH_SIZE] # average the fake output\n","                    # pass MICROBATCH through generator: making sure data.size(0) evenly divisible by PAC (=10)\n","                    fake_data = self(data_micro)\n","                    # pass MICROBATCH through discriminator \n","                    y_fake = self.discriminator(fake_data) \n","\n","                    # compute MICROBATCH loss and attach grads                     \n","                    loss_g = -torch.mean(F.softplus(y_fake))\n","                    h_A = self.generator.h_func()\n","                    \n","                    l2_reg = 0.5 * self.mul2 * self.generator.l2_reg()\n","                    l1_reg = self.mul1 * self.generator.fc1_l1_reg()\n","                    \n","                    loss_g += lambda_A * h_A + 0.5 * c_A * h_A * h_A \n","                    loss_g += l2_reg + l1_reg\n","\n","                    loss_g.backward(retain_graph=True)\n","\n","                    # clip them\n","                    torch.nn.utils.clip_grad_norm_(self.generator.parameters(), self.MAX_GRAD_NORM)\n","                    for name, param in self.generator.named_parameters():\n","                        clipped_gen_grads[name] += param.grad\n","                    # grads saved in clipped_grads, so remove from params\n","                    self.generator.zero_grad()\n","\n","                # last step: now attach the clipped gradients and add noise to them\n","                for name, param in self.generator.named_parameters():\n","                    param.grad = (clipped_gen_grads[name] + torch.FloatTensor(\n","                        clipped_gen_grads[name].size()).normal_(0, self.SIGMA * self.MAX_GRAD_NORM).cuda()) / (\n","                                              data.size(0) / self.MICRO_BATCH_SIZE)\n","               \n","                # update parameters with privatised (bounded) gradients \n","                optimizerG.step() \n","\n","                # increment counter for the #times we update based on privatised grads\n","                self.priv_steps += 1\n","\n","            else:\n","                        \n","                fake_data = self(data)\n","                \n","                y_fake = self.discriminator(fake_data) \n","                \n","                loss_g = -torch.mean(F.softplus(y_fake))\n","                \n","                h_A = self.generator.h_func()\n","                \n","                l2_reg = 0.5 * self.mul2 * self.generator.l2_reg()\n","                l1_reg = self.mul1 * self.generator.fc1_l1_reg()\n","                \n","                loss_g += lambda_A * h_A + 0.5 * c_A * h_A * h_A \n","                \n","                loss_g += l2_reg + l1_reg\n","                \n","                loss_g.backward()\n","                loss_g = optimizerG.step() \n","            \n","            # compute metrics\n","            graph = self.generator.fc1_to_adj()\n","            graph[np.abs(graph) < self.graph_threshold] = 0\n","                 \n","            if ground_truth_G != None:\n","                fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n","                shd_trian.append(shd)\n","                \n","            # mse_train.append(F.mse_loss(fake_data, data_micro.squeeze()).item())\n","            #nll_train.append(loss_g.item())\n","            #kl_train.append(loss_d.item())\n","\n","        ''' update learning rates \n","        '''\n","        self.schedulerG.step()\n","        self.schedulerD.step()\n","\n","        ''' track metrics for differential privacy\n","        '''\n","        print('\\ttime to update models: ',time.time() - t)\n","        t_priv = time.time()\n","        if self.differentialPrivacy:\n","            # Calculate the current privacy cost using the accountant: \n","            max_lmbd = 1023\n","            lmbds = range(2, max_lmbd + 1)          \n","            print('\\tcumulative # param updates: ', self.priv_steps)\n","            # Moments accountant: TensorFlow implementation (see RDP accountant block):\n","            rdp = compute_rdp(self.batch_size / len(train_loader.dataset), self.SIGMA, self.priv_steps, lmbds)\n","            self.eps, _, _ = get_privacy_spent(lmbds, rdp, target_delta=self.DELTA)\n","            print('\\ttime to compute privacy budget: ',time.time() - t_priv)\n","\n","        if ground_truth_G != None:\n","            \n","            print('Epoch: {:04d}'.format(epoch),\n","                  'nll_train: {:.10f}'.format(np.mean(nll_train)),\n","                  'kl_train: {:.10f}'.format(np.mean(kl_train)),\n","                  'ELBO_loss: {:.10f}'.format(np.mean(kl_train)  + np.mean(nll_train)),\n","                  'mse_train: {:.10f}'.format(np.mean(mse_train)),\n","                  'shd_trian: {:.10f}'.format(np.mean(shd_trian)),\n","                  'time: {:.4f}s'.format(time.time() - t),\n","                  # 'epsilon: {:.4f}/{}'.format(self.eps, self.EPSILON)\n","                  )\n","            return self.eps, np.mean(np.mean(kl_train)  + np.mean(nll_train)), np.mean(nll_train), np.mean(mse_train), graph#, origin_A\n","        else:\n","            print('Epoch: {:04d}'.format(epoch),\n","                  'nll_train: {:.10f}'.format(np.mean(nll_train)),\n","                  'kl_train: {:.10f}'.format(np.mean(kl_train)),\n","                  'ELBO_loss: {:.10f}'.format(np.mean(kl_train)  + np.mean(nll_train)),\n","                  'mse_train: {:.10f}'.format(np.mean(mse_train)),\n","                  'time: {:.4f}s'.format(time.time() - t),\n","                  # 'epsilon: {:.4f}/{}'.format(self.eps, self.EPSILON)\n","                  )\n","            return self.eps, np.mean(np.mean(kl_train)  + np.mean(nll_train)), np.mean(nll_train), np.mean(mse_train), graph#, origin_A \n","    \n","    def fit(self, train_loader, ground_truth_G = None):\n","        \n","        if not hasattr(self, \"discriminator\"):\n","            self.discriminator = Discriminator(self.data_variable_size, (256, 256), self.negative_slope, self.dropout_rate).double().to(self.device)\n","            \n","        if not hasattr(self, \"generator\"):\n","            self.generator = Generator(self.z_dims, dims=[self.data_variable_size, 10, 1], bias=True).double().to(self.device)\n","            \n","        if not hasattr(self, \"optimizerD\"):\n","            self.optimizerD = optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(0.5, 0.9), weight_decay=1e-6)\n","            \n","        if not hasattr(self, \"optimizerG\"):\n","            self.optimizerG = optim.Adam(self.generator.parameters(), lr=self.lr)\n","            \n","        if not hasattr(self, \"schedulerG\"):\n","            self.schedulerG = lr_scheduler.StepLR(self.optimizerG, step_size=self.lr_decay, gamma=self.gamma)\n","            \n","        if not hasattr(self, \"schedulerD\"):\n","            self.schedulerD = lr_scheduler.StepLR(self.optimizerD, step_size=self.lr_decay, gamma=self.gamma)\n","\n","        best_ELBO_loss = np.inf\n","        best_NLL_loss = np.inf\n","        best_MSE_loss = np.inf\n","        best_epoch = 0\n","        best_ELBO_graph = []\n","        best_NLL_graph = []\n","        best_MSE_graph = []\n","\n","        try:\n","            if args.differentialPrivacy:\n","                epoch = 0\n","                while self.eps < self.EPSILON:\n","                    epoch += 1\n","                    self.eps, ELBO_loss, NLL_loss, MSE_loss, graph = self.train(train_loader,\n","                    epoch, best_ELBO_loss, ground_truth_G, \n","                    self.lambda_A, self.c_A, self.optimizerG, self.optimizerD)\n","                    \n","                    #increment counter\n","                    print('\\t*******')\n","                    print('\\tprivacy budget expended so far: {:.4f}/{}'.format(self.eps, self.EPSILON))\n","                    print('\\t*******')\n","                    # if epoch%100==0:\n","                    #   exit()\n","                \n","                    if ELBO_loss < best_ELBO_loss:\n","                        best_ELBO_loss = ELBO_loss\n","                        best_epoch = epoch\n","                        best_ELBO_graph = graph\n","\n","                    if NLL_loss < best_NLL_loss:\n","                        best_NLL_loss = NLL_loss\n","                        best_epoch = epoch\n","                        best_NLL_graph = graph\n","\n","                    if MSE_loss < best_MSE_loss:\n","                        best_MSE_loss = MSE_loss\n","                        best_epoch = epoch\n","                        best_MSE_graph = graph\n","                \n","\n","                    # # update parameters\n","                    # # h_A, adj_A are computed in loss anyway, so no need to store\n","                    # self.h_A_old = self.h_A_new\n","                    # self.lambda_A += self.c_A * self.h_A_new\n","                print('\\t*******')\n","                print('\\tprivacy budget reached')\n","                print('\\t*******')\n","  \n","            else:\n","                for step_k in range(self.k_max_iter):\n","                    while self.c_A < 1e+20:\n","                        for epoch in range(self.epochs):\n","                            self.eps, ELBO_loss, NLL_loss, MSE_loss, graph = self.train(train_loader,\n","                            epoch, best_ELBO_loss, ground_truth_G, \n","                            self.lambda_A, self.c_A, self.optimizerG, self.optimizerD)\n","                            if ELBO_loss < best_ELBO_loss:\n","                                best_ELBO_loss = ELBO_loss\n","                                best_epoch = epoch\n","                                best_ELBO_graph = graph\n","\n","                            if NLL_loss < best_NLL_loss:\n","                                best_NLL_loss = NLL_loss\n","                                best_epoch = epoch\n","                                best_NLL_graph = graph\n","\n","                            if MSE_loss < best_MSE_loss:\n","                                best_MSE_loss = MSE_loss\n","                                best_epoch = epoch\n","                                best_MSE_graph = graph\n","\n","                        print(\"Optimization Finished!\")\n","                        print(\"Best Epoch: {:04d}\".format(best_epoch))\n","                    \n","                        if ELBO_loss > 2 * best_ELBO_loss:\n","                            break\n","\n","                        # update parameters\n","                        with torch.no_grad():\n","                            self.h_A_new = self.generator.h_func().item()\n","                        if self.h_A_new > 0.25 * self.h_A_old:\n","                            self.c_A*=10\n","                        else:\n","                            break\n","\n","                    # update parameters\n","                    # h_A, adj_A are computed in loss anyway, so no need to store\n","                    self.h_A_old = self.h_A_new\n","                    self.lambda_A += self.c_A * self.h_A_new\n","                    \n","                    if self.h_A_new <= self.h_tol:\n","                        break\n","                \n","            if ground_truth_G != None:\n","                # test()\n","                #print (best_ELBO_graph)\n","                #print(nx.to_numpy_array(ground_truth_G))\n","                fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_ELBO_graph))\n","                print('Best ELBO Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","                #print(best_NLL_graph)\n","                #print(nx.to_numpy_array(ground_truth_G))\n","                fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_NLL_graph))\n","                print('Best NLL Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","                #print (best_MSE_graph)\n","                #print(nx.to_numpy_array(ground_truth_G))\n","                fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_MSE_graph))\n","                print('Best MSE Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","                graph = self.generator.fc1_to_adj()\n","                graph[np.abs(graph) < 0.1] = 0\n","                # print(graph)\n","                fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n","                print('threshold 0.1, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","                graph[np.abs(graph) < 0.2] = 0\n","                # print(graph)\n","                fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n","                print('threshold 0.2, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","                graph[np.abs(graph) < 0.3] = 0\n","                # print(graph)\n","                fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n","                print('threshold 0.3, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","                \n","                return graph\n","            else:\n","                graph = self.generator.fc1_to_adj()\n","                graph[np.abs(graph) < self.graph_threshold] = 0\n","                return graph\n","\n","        except KeyboardInterrupt:\n","            # print the best anway\n","            #print(best_ELBO_graph)\n","            #print(nx.to_numpy_array(ground_truth_G))\n","            fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_ELBO_graph))\n","            print('Best ELBO Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","            #print(best_NLL_graph)\n","            #print(nx.to_numpy_array(ground_truth_G))\n","            fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_NLL_graph))\n","            print('Best NLL Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","            #print(best_MSE_graph)\n","            #print(nx.to_numpy_array(ground_truth_G))\n","            fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(best_MSE_graph))\n","            print('Best MSE Graph Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","            graph = self.generator.fc1_to_adj()\n","            graph[np.abs(graph) < 0.1] = 0\n","            # print(graph)\n","            fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n","            print('threshold 0.1, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","            graph[np.abs(graph) < 0.2] = 0\n","            # print(graph)\n","            fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n","            print('threshold 0.2, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","\n","            graph[np.abs(graph) < 0.3] = 0\n","            # print(graph)\n","            fdr, tpr, fpr, shd, nnz = count_accuracy(ground_truth_G, nx.DiGraph(graph))\n","            print('threshold 0.3, Accuracy: fdr', fdr, ' tpr ', tpr, ' fpr ', fpr, 'shd', shd, 'nnz', nnz)\n","    \n","    def save_model(self):\n","        assert self.save_directory != '', 'Saving directory not specified! Please specify a saving directory!'\n","        torch.save(self.generator.state_dict(), os.path.join(self.save_directory,'generator.pth'))\n","        torch.save(self.discriminator.state_dict(), os.path.join(self.save_directory,'discriminator.pth'))\n","        \n","    def load_model(self):\n","        assert self.load_directory != '', 'Loading directory not specified! Please specify a loading directory!'\n","        \n","        generator = Generator(self.z_dims, dims=[self.data_variable_size, 10, 1], bias=True).double().to(self.device)\n","        discriminator = Discriminator(self.data_variable_size, (256, 256), self.negative_slope, self.dropout_rate).double().to(self.device)\n","             \n","        generator.load_state_dict(torch.load(os.path.join(self.load_directory,'generator.pth')))\n","        discriminator.load_state_dict(torch.load(os.path.join(self.load_directory,'discriminator.pth')))\n","             \n","        return generator, discriminator\n","\n"],"metadata":{"id":"5VbHHeAan_SD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Private_Main.py"],"metadata":{"id":"X3M1FjVtoAKO"}},{"cell_type":"code","source":["\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat Oct 24 22:49:31 2020\n","@author: Hristo Petkov\n","\"\"\"\n","\n","\"\"\"\n","@inproceedings{yu2019dag,\n","  title={DAG-GNN: DAG Structure Learning with Graph Neural Networks},\n","  author={Yue Yu, Jie Chen, Tian Gao, and Mo Yu},\n","  booktitle={Proceedings of the 36th International Conference on Machine Learning},\n","  year={2019}\n","}\n","@inproceedings{xu2019modeling,\n","  title={Modeling Tabular data using Conditional GAN},\n","  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},\n","  booktitle={Advances in Neural Information Processing Systems},\n","  year={2019}\n","}\n","\"\"\"\n","\n","#Importing libraries and frameworks\n","import time\n","import os\n","import torch\n","import pickle\n","import numpy as np\n","import networkx as nx\n","import pandas as pd \n","# from FullDataPreProcessor import FullDataPreProcessor\n","# from AAE_WGAN_GP import AAE_WGAN_GP\n","# from Utils import load_data\n","# from Utils import draw_dag\n","# from Utils import compute_BiCScore\n","# from Utils import pns_\n","from argparse import ArgumentParser\n","\n","#Adding a lot of args here\n","parser = ArgumentParser()\n","parser.add_argument('--path', type=str, default='',\n","                    help='choosing a path for the input.')\n","parser.add_argument('--column_names_list', type=str, nargs='+', default=[],\n","                    help = 'choosing the column names for samping of original dataframe')\n","parser.add_argument('--discrete_column_names_list', type=str, nargs='+', default=[],\n","                    help = 'choosing the discrete column names in the dataframe')\n","parser.add_argument('--discriminator_steps', type=int, default=1,\n","                    help='Number of steps for the discriminator')\n","parser.add_argument('--initial_identifier', type=str, default='',\n","                    help='Initial Identifier for the sample dataframe')\n","parser.add_argument('--num_of_rows', type=int, default=-1,\n","                    help='Number of rows in the sampled dataframe')\n","parser.add_argument('--save_model', default='', type=str,\n","                        help='A directory to save a trained model to.')\n","parser.add_argument('--load_model', default='', type=str,\n","                    help='A directory to load a trained model from.')\n","parser.add_argument('--export_directory', type=str, default='',\n","                    help='choosing a directory for the output.')\n","parser.add_argument('--verbose', type=int, default=1,\n","                    help='used to control the print statements per epoch.')\n","\n","# -----------data parameters ------\n","# configurations\n","parser.add_argument('--synthesize', type=int, default=0,\n","                    help='Flag for synthesiing synthetic data')\n","parser.add_argument('--pns', type=int, default=1,\n","                    help='Flag for primary neighbour selection')\n","parser.add_argument('--data_type', type=str, default='synthetic',\n","                    choices=['synthetic', 'benchmark', 'real'],\n","                    help='choosing which experiment to do.')\n","parser.add_argument('--data_sample_size', type=int, default=5000,\n","                    help='the number of samples of data')\n","parser.add_argument('--data_variable_size', type=int, default=10,\n","                    help='the number of variables in synthetic generated data')\n","parser.add_argument('--graph_type', type=str, default='erdos-renyi',\n","                    help='the type of DAG graph by generation method')\n","parser.add_argument('--graph_degree', type=int, default=3,\n","                    help='the number of degree in generated DAG graph')\n","parser.add_argument('--graph_sem_type', type=str, default='linear-gauss',\n","                    help='the structure equation model (SEM) parameter type')\n","parser.add_argument('--graph_linear_type', type=str, default='nonlinear_2',\n","                    help='the synthetic data type: linear -> linear SEM, nonlinear_1 -> x=Acos(x+1)+z, nonlinear_2 -> x=2sin(A(x+0.5))+A(x+0.5)+z')\n","parser.add_argument('--edge-types', type=int, default=2,\n","                    help='The number of edge types to infer.')\n","parser.add_argument('--x_dims', type=int, default=1, #vector case: need to be equal to the last dimension of vector data to work\n","                    help='The number of input dimensions: default 1.')\n","parser.add_argument('--z_dims', type=int, default=1,\n","                    help='The number of latent variable dimensions: default the same as variable size.')\n","\n","# -----------training hyperparameters\n","parser.add_argument('--graph_threshold', type=  float, default = 0.3,  # 0.3 is good, 0.2 is error prune\n","                    help = 'threshold for learned adjacency matrix binarization')\n","parser.add_argument('--tau_A', type = float, default=0.0,\n","                    help='coefficient for L-1 norm of A.')\n","parser.add_argument('--lambda_A',  type = float, default= 0.,\n","                    help='coefficient for DAG constraint h(A).')\n","parser.add_argument('--c_A',  type = float, default= 1,\n","                    help='coefficient for absolute value h(A).')\n","parser.add_argument('--negative_slope', type=float, default=0.2,\n","                    help='negative_slope for leaky_relu')\n","parser.add_argument('--dropout_rate', type=float, default=0.0,\n","                    help='rate for discriminator dropout')\n","parser.add_argument('--noise', type=float, default=0.5,\n","                    help='amount of noise for the ANM')\n","\n","\n","parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n","parser.add_argument('--epochs', type=int, default= 300,\n","                    help='Number of epochs for step 1 to train.')\n","parser.add_argument('--epochs2', type=int, default= 600,\n","                    help='Number of epochs for step 2 to train.')\n","parser.add_argument('--batch_size', type=int, default = 100, # note: should be divisible by sample size, otherwise throw an error\n","                    help='Number of samples per batch.')\n","parser.add_argument('--lr', type=float, default=3e-3,  # basline rate = 1e-3\n","                    help='Initial learning rate.')\n","parser.add_argument('--encoder-hidden', type=int, default=64,\n","                    help='Number of hidden units.')\n","parser.add_argument('--decoder-hidden', type=int, default=64,\n","                    help='Number of hidden units.')\n","parser.add_argument('--k_max_iter', type = int, default = 1e2,\n","                    help ='the max iteration number for searching lambda and c')\n","parser.add_argument('--mul1', default=0.01, type=float,\n","                    help='multiplier for the L1_Loss')\n","parser.add_argument('--mul2', default=0.01, type=float,\n","                    help='multiplier for the L2_Loss')\n","\n","parser.add_argument('--suffix', type=str, default='_springs5',\n","                    help='Suffix for training data (e.g. \"_charged\".')\n","parser.add_argument('--h_tol', type=float, default = 1e-8,\n","                    help='the tolerance of error of h(A) to zero')\n","parser.add_argument('--lr-decay', type=int, default=200,\n","                    help='After how epochs to decay LR by a factor of gamma.')\n","parser.add_argument('--gamma', type=float, default= 1.0,\n","                    help='LR decay factor.')\n","parser.add_argument('--temp', type=float, default=1.0,\n","                    help='Temperature for Gumbel softmax.')\n","parser.add_argument('--hard', action='store_true', default=False,\n","                    help='Uses discrete samples in training forward pass.')\n","\n","\n","#******************************************************************\n","# hyper-parameters for differential privacy. \n","parser.add_argument('--SIGMA', type=float, default = 0.5,\n","                    help='Amount of noise to add to CLIPPED gradients during DP-SGD. Larger means more private, so it will take longer to train until reaching EPSILON.')\n","parser.add_argument('--DELTA', type=float, default = 1e-5,\n","                    help='Additional term for widening privacy bound set by EPSILON. Smaller means tighter bound, so more private. Typically set to 1/N, where N is the dataset size.')\n","parser.add_argument('--EPSILON', type=float, default=50.0,\n","                    help='Privacy budget, which sets an upper bound on how much model performance will change with and without new data. Lower means more private, but poorer performance.')\n","parser.add_argument('--MAX_GRAD_NORM', type=float, default= 0.1,\n","                    help='How much we clip the gradients by during each optim.step(). Lower means greater privacy, since we allow parameters to learn less info from the data.')\n","parser.add_argument('--MICRO_BATCH_SIZE', type=int, default=10,\n","                    help='Number of samples to average gradients over during optim.step(). Keep as small as possible, but also divisible by BATCH_SIZE.')\n","parser.add_argument('--differentialPrivacy', type=bool, default=True,\n","                    help='Choose whether or not to privatise SGD.')\n","# ******************************************************************\n","\n","# Line below NEEDED in Colab. Otherwise, argparse doesnt work...\n","parser.add_argument(\"-f\", \"--file\", required=False)\n","args = parser.parse_args()\n","print(args)\n","\n","#controlls randomness of the entire program\n","torch.manual_seed(args.seed)\n","\n","def main():\n","    \n","    t = time.time()\n","    \n","    if args.data_type == 'real':\n","        \n","        train_loader, data_variable_size, columns = load_data(args, args.batch_size, args.suffix)\n","        \n","        # add adjacency matrix A\n","        num_nodes = data_variable_size\n","        adj_A = np.zeros((num_nodes, num_nodes))\n","    \n","        aae_wgan_gp = AAE_WGAN_GP(args, adj_A)\n","        \n","        causal_graph = aae_wgan_gp.fit(train_loader)\n","        \n","        draw_dag(causal_graph, args.data_type, columns)\n","        \n","    elif args.data_type == 'benchmark':\n","        \n","        train_loader, data_variable_size, ground_truth_G, num_cats  = load_data(args, args.batch_size, args.suffix)\n","        \n","         # add adjacency matrix A\n","        num_nodes = data_variable_size\n","        adj_A = np.zeros((num_nodes, num_nodes))\n","    \n","        aae_wgan_gp = AAE_WGAN_GP(args, adj_A)\n","        \n","        causal_graph = aae_wgan_gp.fit(train_loader, ground_truth_G)\n","        \n","        BIC_score = compute_BiCScore(np.asarray(nx.to_numpy_matrix(ground_truth_G)), causal_graph)\n","        print('BIC_score: ' + str(BIC_score))\n","        \n","        draw_dag(causal_graph, args.data_type)\n","        \n","    else:\n","        if args.synthesize:\n","            #create and store synthetic data\n","            train_loader, ground_truth_G = load_data(args, args.batch_size, args.suffix)\n","            \n","            with open(r\"train_loader.pkl\", \"wb\") as output_file:\n","                pickle.dump(train_loader, output_file)\n","        \n","            with open(r\"ground_truth_G.pkl\", \"wb\") as output_file_G:\n","                pickle.dump(ground_truth_G, output_file_G)\n","                \n","        #load synthetic data\n","        with open(r\"train_loader.pkl\", \"rb\") as input_file:\n","            train_data = pickle.load(input_file)\n","        \n","        with open(r\"ground_truth_G.pkl\", \"rb\") as input_file_G:\n","            ground_truth = pickle.load(input_file_G)\n","        \n","        # add adjacency matrix A\n","        num_nodes = args.data_variable_size\n","        adj_A = np.zeros((num_nodes, num_nodes))\n","        \n","        #test = pns_(adj_A, train_data, num_nodes, 0.75)\n","        #print(test)\n","        #print(test.shape)\n","    \n","        aae_wgan_gp = AAE_WGAN_GP(args, adj_A)\n","        causal_graph = aae_wgan_gp.fit(train_data, ground_truth)\n","        \n","        #draw_dag(causal_graph, args.data_type)\n","    \n","    #causal_graph.to_csv(os.path.join(args.export_directory, 'adjacency_matrix.csv'), index=False)\n","                \n","    print('Programm finished in: ' + str(time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - t))))\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"],"metadata":{"id":"_yIKAJvGoC9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#========================================="],"metadata":{"id":"x3jf82FboFQi"}},{"cell_type":"markdown","source":["# Thoughts on DAG-WGAN implementation\n","\n","Usually, we would only need to privatise updates when the parameters have been involved with the real data. Intuitively, we don't need to worry about the privacy of fake patients, but rather the real ones! \n","For this reason, when a set of parameters are learning information from the real data, we need to constrain their gradients to limit how much they learn from the real data.\n","\n","This would be the case if $z\\thicksim\\mathcal{N}$, as in GANs (i.e., generate X, given random noise).\n","\n","**However, with DAG-WGAN, because the fake data is produced as a function of the real features from the encoder $E$ i.e., $z\\thicksim P_E(z|X)$, then this means we probably need to bound all gradients in the model**\n","\n","In other words, since every part of this model will see the patient data, their parameters need to be constrained. \n","\n","# Components\n","- $E: Xz$. Maps the patient data to their corresponding latent representation. The encoder sees the patient's data directly, and therefore must be privatised.\n","- $G: z\\hat{X}$. Generator for reconstructing $X$ from its latent features (and $A$: parametric case). Since $z$ contains information about $X$, we will also need to privatise updates made to $G$.\n","- $D: \\mathbb{R}^{|X|}\\rightarrow\\mathbb{R}$. Maps given data to real-value (i.e. probability of real/fake). Both sets of data ($X, \\hat{X}$) contain patient info, so all updates to $D$ need to be privatised too. "],"metadata":{"id":"g69Uumd5CcLR"}},{"cell_type":"markdown","source":["## Template\n","This isn't a working version, but a simple pseudocode-type illustration for how differential privacy would work with plain SGD (DP-SGD). "],"metadata":{"id":"2V4ArpaKzgJV"}},{"cell_type":"code","source":["from torch.nn.utils import clip_grad_norm_\n","\n","for batch in Dataloader(train_dataset, batch_size=100):\n","    for param in model.parameters():\n","        param.accumulated_grads = []\n","    \n","    # Run the microbatches: these are samples from the mini-batch (e.g. = 10).\n","    # For example: given a MINI-batch of 100 samples, a MICRO-batch could be 10 of these samples. \n","    #              we would then compute the avg gradient over these 10 samples (instead of the 100 as we usually do).\n","    for microbatch in batch:\n","        x, y = microbatch\n","        y_hat = model(x)\n","        loss = criterion(y_hat, y)\n","        loss.backward()\n","    \n","        # Clip each parameter's per-sample gradient\n","        for param in model.parameters():\n","            per_sample_grad = p.grad.detach().clone()\n","            clip_grad_norm_(per_sample_grad, max_norm=args.max_grad_norm)  \n","            param.accumulated_grads.append(per_sample_grad)  \n","        \n","    # Aggregate back\n","    for param in model.parameters():\n","        param.grad = torch.stack(param.accumulated_grads, dim=0)\n","\n","    # Now we are ready to update and add noise!\n","    for param in model.parameters():\n","        param = param - args.lr * param.grad    # plain SGD, without momentum. \n","        param += torch.normal(mean=0, std=args.noise_multiplier * args.max_grad_norm)\n","        \n","    model.zero_grads()"],"metadata":{"id":"1p7ZopT78Z14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Practice stuff\n","Place to play around with code snippets before merging with main"],"metadata":{"id":"8VEZQ8Vdb2uM"}},{"cell_type":"code","source":[],"metadata":{"id":"06UvDxRRLM_Y"},"execution_count":null,"outputs":[]}]}